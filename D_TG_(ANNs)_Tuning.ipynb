{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUILLENJV/Optimizaci-n-de-Campa-as/blob/master/D_TG_(ANNs)_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndwgPlN4HEto"
      },
      "source": [
        "# Artificial Neural Networks (ANNs) Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAMDsS8hroPR",
        "outputId": "84c0601a-1245-49f9-9c53-42a5dc6a06e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.4.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEN_sUdX19iW"
      },
      "outputs": [],
      "source": [
        "# example of learned embedding encoding for a neural network\n",
        "from numpy import unique\n",
        "import numpy\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.regularizers import l1\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "\n",
        "#from predict import model\n",
        "from sklearn import preprocessing\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFcJ0upZ-tcq"
      },
      "source": [
        "# Generando el modelo con los parametros calibrados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkDDVvzXBriU"
      },
      "outputs": [],
      "source": [
        "from numpy import unique\n",
        "import numpy\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "#import datetime\n",
        "#from time import sleep\n",
        "#from tqdm import tqdm\n",
        "#import os\n",
        "from pandas import read_csv\n",
        "#import json\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "#from keras.utils import np_utils\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "#import requests\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,OrdinalEncoder\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.regularizers import l1, l2\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "#from tensorflow.keras.callbacks import EarlyStopping\n",
        "#############################################\n",
        "from tensorflow.keras import regularizers\n",
        "#from predict import model\n",
        "#from sklearn import preprocessing\n",
        "#from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "####################################################\n",
        "#from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from matplotlib import pyplot as plt\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from matplotlib import pyplot\n",
        "import seaborn as sns\n",
        "####################################################\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBUdpQopdBuW",
        "outputId": "b3370864-a8fb-4901-ee07-1746e6b637b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"62f1ab79cc287b001f643389_clear_final.csv\")\n",
        "X = df[['campaign_id',\n",
        "        'traffic_source_id',\n",
        "        'visitor_device_browser',\n",
        "        'visitor_device_hardware_family',\n",
        "        'visitor_device_hardware_model',\n",
        "        'visitor_device_hardware_vendor',\n",
        "        'visitor_device_os_family',\n",
        "        'visitor_device_os_vendor',\n",
        "        'visitor_device_os_version',\n",
        "        'visitor_device_type',\n",
        "        'visitor_geo_location_cityName',\n",
        "        'visitor_geo_location_connection_typ',\n",
        "        'visitor_geo_location_countryCode',\n",
        "        'visitor_geo_location_isp',\n",
        "        'visitor_geo_location_regionName',\n",
        "        'visitor_tokens_adh',\n",
        "        'visitor_tokens_cadid',\n",
        "        'visitor_tokens_adi',\n",
        "        'converted_yes',\n",
        "        'converted_no']]\n",
        "y = df[[\"landing_page_id\"]]\n",
        "\n",
        "df = pd.concat([X, y], axis=1)\n",
        "\n",
        "# prepare input data\n",
        "def prepare_inputs(X_train, X_test):\n",
        "\toe = OneHotEncoder(handle_unknown='ignore')\n",
        "\toe.fit(X_train)\n",
        "\tX_train_enc = oe.transform(X_train)\n",
        "\tX_test_enc = oe.transform(X_test)\n",
        "\treturn X_train_enc, X_test_enc\n",
        "\n",
        "# prepare target\n",
        "def prepare_targets(y_train, y_test):\n",
        "  le = LabelEncoder()\n",
        "  le.fit(y_train)\n",
        "  y_train_enc = le.transform(y_train)\n",
        "  y_test_enc = le.transform(y_test)\n",
        "  y_train_enc = to_categorical(y_train_enc)\n",
        "  y_test_enc = to_categorical(y_test_enc)\n",
        "  return y_train_enc, y_test_enc\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
        "\n",
        "# prepare input data\n",
        "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
        "\n",
        "# prepare output data\n",
        "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset with grid search via sklearn\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XS2cS1ehjF6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DL_Model(activation= 'relu', neurons= 1, optimizer='Adam', init='uniform'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(250, input_dim=X_train_enc.shape[1], activation=activation,\n",
        "                    kernel_initializer = init))\n",
        "    model.add(Dense(250, kernel_initializer = init,  activation = activation))\n",
        "    model.add(Dense(2, kernel_initializer = init, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=DL_Model, epochs= 10, batch_size=16, verbose= 0)\n",
        "print(model.get_params().keys())\n",
        "\n",
        "# Definying grid parameters\n",
        "init = ['uniform', 'lecun_uniform', 'glorot_uniform','he_uniform']\n",
        "activation = ['softmax', 'relu', 'tanh', 'sigmoid']\n",
        "neurons = [1,2,3,4,5]\n",
        "optimizer = ['RMSprop', 'Adagrad', 'Adadelta', 'Adam']\n",
        "epochs = [10,20,30]\n",
        "batches = [16, 32, 64]\n",
        "param_grid = dict(model__activation=activation, model__neurons=neurons,\n",
        "                  model__init=init,\n",
        "                  epochs=epochs, batch_size=batches, optimizer = optimizer)\n",
        "model = GridSearchCV(estimator= model, param_grid=param_grid, n_jobs=-1)\n",
        "model.fit(X_train_enc,y_train_enc)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (model.best_score_, model.best_params_))\n",
        "means = model.cv_results_['mean_test_score']\n",
        "stds = model.cv_results_['std_test_score']\n",
        "params = model.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpZZLYKjGM9",
        "outputId": "92e84d85-7ed1-43be-98ac-fa2c12f39e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'class_weight'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}